# ğŸ§  ai_ollama

ğŸ§ª **Sandbox for experimenting with [Ollama](https://ollama.com/)** â€” a local LLM (Large Language Model) runner, integrated with PostgreSQL.

---

## âš™ï¸ Provision the solition 
```bash
- cd pgdb
- vagrant up
- vagrant ssh 
```
After running vagrant up, a VM is created with:
	â€¢	Ubuntu 20.04 (ubuntu/focal64)
	â€¢	PostgreSQL 16 installed and configured to accept remote connections (host: 6432)
	â€¢	Installed extensions:
	â€¢	pgvector â€” for vector similarity search
	â€¢	plpython3u â€” to run Python-based functions inside PostgreSQL

Inside the VM:
	â€¢	A database testdb is created and marat user is created with superuser access
	â€¢	A commit_logs table is created with an embedding column of type vector(1024)
	â€¢	Commits from postgres/postgres GitLab [repo](https://gitlab.com/postgres/postgres.git) are loaded into the table
	â€¢	Initial SQL logic from [query.sql](query.sql) is applied

Inside the VM:
- Follow the steps in [pgdb/how_prepare_postgres.md](pgdb/how_prepare_postgres.md)
- Then run Ollama [via Docker](ollama/llama3.md)

## ğŸ§ª Usage Example
Once provisioned, you can run SQL queries like:
```sql
select local_chat('Optimise numeric multiplication for short inputs. â€“ when, who, etc.');
and olamma will provide the answer  
```
And Ollama will generate an answer using a local model.

## ğŸ“‚ Structure
- pgdb/ â€“ Vagrant environment with PostgreSQL setup
- ollama/ â€“ Docker-related instructions for Ollama